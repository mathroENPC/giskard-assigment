{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pvKxJjsNSoE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![giskard_logo.png](https://raw.githubusercontent.com/Giskard-AI/giskard/main/readme/Logo_full_darkgreen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a9b17467105f4031a3f9eae70ef4138f",
    "deepnote_cell_height": 134,
    "deepnote_cell_type": "markdown",
    "id": "PKcOi3D37xbW",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# About Giskard\n",
    "\n",
    "Open-Source CI/CD platform for ML teams. Deliver ML products, better & faster. \n",
    "\n",
    "*   Collaborate faster with feedback from business stakeholders.\n",
    "*   Deploy automated tests to eliminate regressions, errors & biases.\n",
    "\n",
    "沛｡ [Website](https://giskard.ai/)\n",
    "\n",
    "沒 [Documentation](https://docs.giskard.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing `giskard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting giskard\n",
      "  Downloading giskard-1.8.0-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m69.7/69.7 KB\u001b[0m \u001b[31m574.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.2\n",
      "  Downloading pydantic-1.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting grpcio<2.0.0,>=1.46.3\n",
      "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.11.1 in ./.local/lib/python3.10/site-packages (from giskard) (4.11.2)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting protobuf<4.0.0,>=3.9.2\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools<68.0.0,>=65.4.1\n",
      "  Downloading setuptools-67.4.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.16.0,>=0.15.2\n",
      "  Downloading zstandard-0.15.2.tar.gz (1.0 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas<2.0.0,>=1.3.5\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-daemon<3.0.0,>=2.3.1\n",
      "  Downloading python_daemon-2.3.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-toolbelt<0.10.0,>=0.9.1\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m54.3/54.3 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.8,>=1.7.2\n",
      "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.22.0,>=1.21.6\n",
      "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting eli5<0.14.0,>=0.13.0\n",
      "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m216.2/216.2 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mixpanel<5.0.0,>=4.10.0\n",
      "  Downloading mixpanel-4.10.0-py2.py3-none-any.whl (8.9 kB)\n",
      "Collecting scikit-learn<1.1.0,>=1.0.0\n",
      "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-status<2.0.0,>=1.46.3\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "Collecting cloudpickle<3.0.0,>=2.1.0\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting ipython<8.0.0,>=7.0.0\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m793.8/793.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.64.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting shap<0.42.0,>=0.41.0\n",
      "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m572.6/572.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil<6.0.0,>=5.9.2 in ./.local/lib/python3.10/site-packages (from giskard) (5.9.4)\n",
      "Collecting click<9.0.0,>=8.1.3\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib_metadata<5.0.0,>=4.11.4\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Collecting lockfile<0.13.0,>=0.12.2\n",
      "  Downloading lockfile-0.12.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting requests<3.0.0,>=2.28.1\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in ./.local/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.11.1->giskard) (2.4)\n",
      "Requirement already satisfied: attrs>17.1.0 in ./.local/lib/python3.10/site-packages (from eli5<0.14.0,>=0.13.0->giskard) (22.2.0)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m995.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=3.0.0 in ./.local/lib/python3.10/site-packages (from eli5<0.14.0,>=0.13.0->giskard) (3.1.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from eli5<0.14.0,>=0.13.0->giskard) (1.16.0)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.46.3\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Collecting googleapis-common-protos>=1.5.5\n",
      "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m223.0/223.0 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib_metadata<5.0.0,>=4.11.4->giskard) (1.0.0)\n",
      "Requirement already satisfied: pygments in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (2.14.0)\n",
      "Requirement already satisfied: decorator in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (3.0.38)\n",
      "Requirement already satisfied: traitlets>=4.2 in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (5.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (0.18.2)\n",
      "Requirement already satisfied: backcall in ./.local/lib/python3.10/site-packages (from ipython<8.0.0,>=7.0.0->giskard) (0.2.0)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<2.0.0,>=1.3.5->giskard) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.5->giskard) (2.8.2)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting docutils\n",
      "  Downloading docutils-0.19-py3-none-any.whl (570 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m570.5/570.5 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.28.1->giskard) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m198.8/198.8 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>20.9 in ./.local/lib/python3.10/site-packages (from shap<0.42.0,>=0.41.0->giskard) (23.0)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.local/lib/python3.10/site-packages (from jedi>=0.16->ipython<8.0.0,>=7.0.0->giskard) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2>=3.0.0->eli5<0.14.0,>=0.13.0->giskard) (2.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.10/site-packages (from pexpect>4.3->ipython<8.0.0,>=7.0.0->giskard) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.0.0->giskard) (0.2.6)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: eli5, zstandard\n",
      "  Building wheel for eli5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107747 sha256=e449d91089ee4ebe44bfc1a33383f1ac8cc1a8eeec046285fa0640deab3e2ffd\n",
      "  Stored in directory: /home/mathro/.cache/pip/wheels/b8/58/ef/2cf4c306898c2338d51540e0922c8e0d6028e07007085c0004\n",
      "  Building wheel for zstandard (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zstandard: filename=zstandard-0.15.2-cp310-cp310-linux_x86_64.whl size=1925510 sha256=0329e8ce90579e7b7fdd85628af5c3eb2e2bb2ab6a56749a87145e29d3b97ab5\n",
      "  Stored in directory: /home/mathro/.cache/pip/wheels/3e/81/cc/f03f726cef9e57238d1ad33f9c3e349f0180d35c6066ef7a5a\n",
      "Successfully built eli5 zstandard\n",
      "Installing collected packages: lockfile, charset-normalizer, zstandard, urllib3, typing-extensions, tqdm, threadpoolctl, tenacity, tabulate, slicer, setuptools, protobuf, numpy, llvmlite, joblib, importlib_metadata, grpcio, graphviz, docutils, cloudpickle, click, certifi, scipy, requests, python-daemon, pydantic, pandas, numba, ipython, googleapis-common-protos, scikit-learn, requests-toolbelt, mixpanel, grpcio-status, shap, eli5, giskard\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.11.0\n",
      "    Uninstalling ipython-8.11.0:\n",
      "      Successfully uninstalled ipython-8.11.0\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-3.0.1 click-8.1.3 cloudpickle-2.2.1 docutils-0.19 eli5-0.13.0 giskard-1.8.0 googleapis-common-protos-1.58.0 graphviz-0.20.1 grpcio-1.51.3 grpcio-status-1.48.2 importlib_metadata-4.13.0 ipython-7.34.0 joblib-1.2.0 llvmlite-0.39.1 lockfile-0.12.2 mixpanel-4.10.0 numba-0.56.4 numpy-1.21.6 pandas-1.5.3 protobuf-3.20.3 pydantic-1.10.5 python-daemon-2.3.2 requests-2.28.2 requests-toolbelt-0.9.1 scikit-learn-1.0.2 scipy-1.7.3 setuptools-67.4.0 shap-0.41.0 slicer-0.0.7 tabulate-0.9.0 tenacity-8.2.2 threadpoolctl-3.1.0 tqdm-4.64.1 typing-extensions-4.5.0 urllib3-1.26.14 zstandard-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install giskard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the external worker in daemon mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-05 20:41:51,220 pid:1447 MainThread giskard.cli  INFO     Starting ML Worker client daemon\r\n",
      "2023-03-05 20:41:51,220 pid:1447 MainThread giskard.cli  INFO     Python: /usr/bin/python3 (3.10.6)\r\n",
      "2023-03-05 20:41:51,220 pid:1447 MainThread giskard.cli  INFO     Giskard Home: /home/mathro/giskard-home\r\n",
      "2023-03-05 20:41:51,221 pid:1447 MainThread giskard.cli_utils INFO     Writing logs to /home/mathro/giskard-home/run/ml-worker.log\r\n"
     ]
    }
   ],
   "source": [
    "!giskard worker start -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f35c8e8d3fbf4c0f9c01a69673c318a1",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 6
    },
    "deepnote_cell_height": 110,
    "deepnote_cell_type": "markdown",
    "id": "mJTqM-W_7xbW",
    "owner_user_id": "41ec0844-b5b7-49c2-9460-710a452f98de",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Start by creating an ML model 泅泅泅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e8d609f32d5243dd917cc3104599b8d8",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 12
    },
    "deepnote_cell_height": 230,
    "deepnote_cell_type": "markdown",
    "id": "WNI85koE7xbX",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Let's create a credit scoring Model using the German Credit scoring dataset [(Link](https://github.com/Giskard-AI/giskard-client/tree/main/sample_data/classification) to download the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "cell_id": "1d7f5390-3fb1-47b9-8488-7a55e5b465f9",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 18
    },
    "deepnote_cell_height": 315,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 798,
    "execution_start": 1652125164139,
    "id": "xEfe0KIo7xbX",
    "pycharm": {
     "name": "#%%\n"
    },
    "source_hash": "e81d6069",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To download and read the credit scoring dataset\n",
    "url = 'https://raw.githubusercontent.com/Giskard-AI/examples/main/datasets/credit_scoring_classification_model_dataset/german_credit_prepared.csv'\n",
    "credit = pd.read_csv(url, sep=',',engine=\"python\") #To download go to https://github.com/Giskard-AI/giskard-client/tree/main/sample_data/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Declare the type of each column in the dataset(example: category, numeric, text)\n",
    "column_types = {'default':\"category\",\n",
    "               'account_check_status':\"category\", \n",
    "               'duration_in_month':\"numeric\",\n",
    "               'credit_history':\"category\",\n",
    "               'purpose':\"category\",\n",
    "               'credit_amount':\"numeric\",\n",
    "               'savings':\"category\",\n",
    "               'present_employment_since':\"category\",\n",
    "               'installment_as_income_perc':\"numeric\",\n",
    "               'sex':\"category\",\n",
    "               'personal_status':\"category\",\n",
    "               'other_debtors':\"category\",\n",
    "               'present_residence_since':\"numeric\",\n",
    "               'property':\"category\",\n",
    "               'age':\"numeric\",\n",
    "               'other_installment_plans':\"category\",\n",
    "               'housing':\"category\",\n",
    "               'credits_this_bank':\"numeric\",\n",
    "               'job':\"category\",\n",
    "               'people_under_maintenance':\"numeric\",\n",
    "               'telephone':\"category\",\n",
    "               'foreign_worker':\"category\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature_types is used to declare the features the model is trained on\n",
    "feature_types = {i:column_types[i] for i in column_types if i!='default'}\n",
    "\n",
    "# Pipeline to fill missing values, transform and scale the numeric columns\n",
    "columns_to_scale = [key for key in feature_types.keys() if feature_types[key]==\"numeric\"]\n",
    "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Pipeline to fill missing values and one hot encode the categorical values\n",
    "columns_to_encode = [key for key in feature_types.keys() if feature_types[key]==\"category\"]\n",
    "categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore',sparse=False)) ])\n",
    "\n",
    "# Perform preprocessing of the columns with the above pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, columns_to_scale),\n",
    "      ('cat', categorical_transformer, columns_to_encode)\n",
    "          ]\n",
    ")\n",
    "\n",
    "# Pipeline for the model Logistic Regression\n",
    "clf_logistic_regression = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(max_iter =1000))])\n",
    "\n",
    "# Split the data into train and test\n",
    "Y=credit['default']\n",
    "X= credit.drop(columns=\"default\")\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.20,random_state = 30, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and score your model\n",
    "clf_logistic_regression.fit(X_train, Y_train)\n",
    "clf_logistic_regression.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data to upload on Giskard\n",
    "train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "test_data = pd.concat([X_test, Y_test ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Upload the model in Giskard 泅泅泅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initiate a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from giskard import GiskardClient\n",
    "\n",
    "url = \"http://localhost:19000\" #if Giskard is installed locally (for installation, see: https://docs.giskard.ai/start/guides/installation)\n",
    "#url = \"http://app.giskard.ai\" # If you want to upload on giskard URL\n",
    "token = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsInRva2VuX3R5cGUiOiJBUEkiLCJhdXRoIjoiUk9MRV9BRE1JTiIsImV4cCI6MTY4NTQ2MDI1OX0.NuosCjh2EhAiCc7d411quTY89bAv8qfBIqpVJD1f6yo\" #you can generate your API token in the Admin tab of the Giskard application (for installation, see: https://docs.giskard.ai/start/guides/installation)\n",
    "\n",
    "client = GiskardClient(url, token)\n",
    "\n",
    "# your_project = client.create_project(\"project_key\", \"PROJECT_NAME\", \"DESCRIPTION\")\n",
    "# Choose the arguments you want. But \"project_key\" should be unique and in lower case\n",
    "# credit_scoring = client.create_project(\"credit_scoring\", \"German Credit Scoring\", \"Project to predict if user will default\")\n",
    "\n",
    "# If you've already created a project with the key \"credit-scoring\" use\n",
    "credit_scoring = client.get_project(\"credit_scoring\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Upload your model and a dataset (see [documentation](https://docs.giskard.ai/start/guides/upload-your-model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 27. It is available at http://localhost:19000 \n",
      "Model successfully uploaded to project key 'credit_scoring' with ID = 28. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 27)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_model_and_df(\n",
    "    prediction_function=clf_logistic_regression.predict_proba, # Python function which takes pandas dataframe as input and returns probabilities for classification model OR returns predictions for regression model\n",
    "    model_type='classification', # \"classification\" for classification model OR \"regression\" for regression model\n",
    "    df=test_data, # the dataset you want to use to inspect your model\n",
    "    column_types=column_types, # A dictionary with columns names of df as key and types(category, numeric, text) of columns as values\n",
    "    target='default', # The column name in df corresponding to the actual target variable (ground truth).\n",
    "    feature_names=list(feature_types.keys()), # List of the feature names of prediction_function\n",
    "    classification_labels=clf_logistic_regression.classes_ ,  # List of the classification labels of your prediction\n",
    "    model_name='logistic_regression_v1', # Name of the model\n",
    "    dataset_name='test_data' # Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 沍 If you want to upload a dataset without a model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For example, let's upload the train set in Giskard, this is key to create drift tests in Giskard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 29. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_df(\n",
    "    df=train_data, # The dataset you want to upload\n",
    "    column_types=column_types, # All the column types of df\n",
    "    target=\"default\", # Do not pass this parameter if dataset doesn't contain target column\n",
    "    name=\"train_data\" # Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also upload new production data to use it as a validation set for your existing model. In that case, you might not have the ground truth target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "production_data = credit.drop(columns=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 30. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_df(\n",
    "    df=production_data, # The dataset you want to upload\n",
    "    column_types=feature_types, # All the column types without the target\n",
    "    name=\"production_data\"# Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 沍 If you just want to upload a model without a dataframe \n",
    "\n",
    "This happens for instance when you built a new version of the model and you want to inspect it using a validation dataframe that is already in Giskard\n",
    "\n",
    "For example, let's create a second version of the model using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_forest = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(max_depth=10,random_state=0))])\n",
    "\n",
    "clf_random_forest.fit(X_train, Y_train)\n",
    "clf_random_forest.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully uploaded to project key 'credit_scoring' with ID = 31. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_model(\n",
    "    prediction_function=clf_random_forest.predict_proba, # Python function which takes pandas dataframe as input and returns probabilities for classification model OR returns predictions for regression model\n",
    "    model_type='classification', # \"classification\" for classification model OR \"regression\" for regression model\n",
    "    feature_names=list(feature_types.keys()), # List of the feature names of prediction_function\n",
    "    name='random_forest', # Name of the model\n",
    "    validate_df=train_data, # Optional. Validation df is not uploaded in the app, it's only used to check whether the model has the good format\n",
    "    target=\"default\", # Optional. target should be a column of validate_df. Pass this parameter only if validate_df is being passed\n",
    "    classification_labels=[\"Default\",\"Not default\"] # List of the classification labels of your prediction\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c49894d61f544e8f88030b7be8078c6b",
    "deepnote_app_coordinates": {
     "h": 5,
     "w": 12,
     "x": 0,
     "y": 96
    },
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "id": "DGQ50rUN7xbe",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Happy Exploration ! 洫鯛昨泅\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 22 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   default                     1000 non-null   object\n",
      " 1   account_check_status        1000 non-null   object\n",
      " 2   duration_in_month           1000 non-null   int64 \n",
      " 3   credit_history              1000 non-null   object\n",
      " 4   purpose                     1000 non-null   object\n",
      " 5   credit_amount               1000 non-null   int64 \n",
      " 6   savings                     1000 non-null   object\n",
      " 7   present_employment_since    1000 non-null   object\n",
      " 8   installment_as_income_perc  1000 non-null   int64 \n",
      " 9   sex                         1000 non-null   object\n",
      " 10  personal_status             1000 non-null   object\n",
      " 11  other_debtors               1000 non-null   object\n",
      " 12  present_residence_since     1000 non-null   int64 \n",
      " 13  property                    1000 non-null   object\n",
      " 14  age                         1000 non-null   int64 \n",
      " 15  other_installment_plans     1000 non-null   object\n",
      " 16  housing                     1000 non-null   object\n",
      " 17  credits_this_bank           1000 non-null   int64 \n",
      " 18  job                         1000 non-null   object\n",
      " 19  people_under_maintenance    1000 non-null   int64 \n",
      " 20  telephone                   1000 non-null   object\n",
      " 21  foreign_worker              1000 non-null   object\n",
      "dtypes: int64(7), object(15)\n",
      "memory usage: 172.0+ KB\n"
     ]
    }
   ],
   "source": [
    "credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_in_month</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>installment_as_income_perc</th>\n",
       "      <th>present_residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>credits_this_bank</th>\n",
       "      <th>people_under_maintenance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.903000</td>\n",
       "      <td>3271.258000</td>\n",
       "      <td>2.973000</td>\n",
       "      <td>2.845000</td>\n",
       "      <td>35.546000</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>1.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.058814</td>\n",
       "      <td>2822.736876</td>\n",
       "      <td>1.118715</td>\n",
       "      <td>1.103718</td>\n",
       "      <td>11.375469</td>\n",
       "      <td>0.577654</td>\n",
       "      <td>0.362086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>1365.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>2319.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>3972.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration_in_month  credit_amount  installment_as_income_perc  \\\n",
       "count        1000.000000    1000.000000                 1000.000000   \n",
       "mean           20.903000    3271.258000                    2.973000   \n",
       "std            12.058814    2822.736876                    1.118715   \n",
       "min             4.000000     250.000000                    1.000000   \n",
       "25%            12.000000    1365.500000                    2.000000   \n",
       "50%            18.000000    2319.500000                    3.000000   \n",
       "75%            24.000000    3972.250000                    4.000000   \n",
       "max            72.000000   18424.000000                    4.000000   \n",
       "\n",
       "       present_residence_since          age  credits_this_bank  \\\n",
       "count              1000.000000  1000.000000        1000.000000   \n",
       "mean                  2.845000    35.546000           1.407000   \n",
       "std                   1.103718    11.375469           0.577654   \n",
       "min                   1.000000    19.000000           1.000000   \n",
       "25%                   2.000000    27.000000           1.000000   \n",
       "50%                   3.000000    33.000000           1.000000   \n",
       "75%                   4.000000    42.000000           2.000000   \n",
       "max                   4.000000    75.000000           4.000000   \n",
       "\n",
       "       people_under_maintenance  \n",
       "count               1000.000000  \n",
       "mean                   1.155000  \n",
       "std                    0.362086  \n",
       "min                    1.000000  \n",
       "25%                    1.000000  \n",
       "50%                    1.000000  \n",
       "75%                    1.000000  \n",
       "max                    2.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default                         2\n",
       "account_check_status            4\n",
       "duration_in_month              33\n",
       "credit_history                  5\n",
       "purpose                        10\n",
       "credit_amount                 921\n",
       "savings                         5\n",
       "present_employment_since        5\n",
       "installment_as_income_perc      4\n",
       "sex                             2\n",
       "personal_status                 3\n",
       "other_debtors                   3\n",
       "present_residence_since         4\n",
       "property                        4\n",
       "age                            53\n",
       "other_installment_plans         3\n",
       "housing                         3\n",
       "credits_this_bank               4\n",
       "job                             4\n",
       "people_under_maintenance        2\n",
       "telephone                       2\n",
       "foreign_worker                  2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=credit.keys().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to select numerical features with high cardinal so : duration_in_month, credit_amount and age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st method - Global Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to choose first between 2 classic methods : Oversampling and Undersampling. I found it more interesting to work with oversampling for this first part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to create new rows with very close values from the 1000 rows using normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_augmented = credit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def random_normal_integer(mean,std):\n",
    "    \"\"\"\n",
    "    returns random integer following normal distribution\n",
    "    \"\"\"\n",
    "    new_value = norm.ppf(np.random.random(1), loc=mean, scale=std).astype(int)[0]\n",
    "    new_value = max(new_value,1)\n",
    "    return(new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_data = {\n",
    "    'duration_in_month':{\n",
    "        \"std\" : credit['duration_in_month'].std().round().astype(int)/10,\n",
    "    },\n",
    "    'credit_amount':{\n",
    "        \"std\" : credit['credit_amount'].std().round().astype(int)/10,\n",
    "    },\n",
    "    'age':{\n",
    "        \"std\" : credit['age'].std().round().astype(int)/10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(row,data_aug_data=data_aug_data):\n",
    "    for el in data_aug_data.keys():\n",
    "        row[el] = random_normal_integer(row[el],data_aug_data[el][\"std\"])\n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_augmented = credit_augmented.apply(data_augmentation,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.concat([credit,credit_augmented],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 719. It is available at http://localhost:19000 \n",
      "Model successfully uploaded to project key 'credit_scoring' with ID = 720. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(720, 719)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "Y=augmented_data['default']\n",
    "X= augmented_data.drop(columns=\"default\")\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.20,random_state = 30, stratify = Y)\n",
    "# Fit and score your model\n",
    "clf_logistic_regression.fit(X_train, Y_train)\n",
    "clf_logistic_regression.score(X_test, Y_test)\n",
    "# Prepare data to upload on Giskard\n",
    "train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "test_data = pd.concat([X_test, Y_test ], axis=1)\n",
    "\n",
    "credit_scoring.upload_model_and_df(\n",
    "    prediction_function=clf_logistic_regression.predict_proba, # Python function which takes pandas dataframe as input and returns probabilities for classification model OR returns predictions for regression model\n",
    "    model_type='classification', # \"classification\" for classification model OR \"regression\" for regression model\n",
    "    df=test_data, # the dataset you want to use to inspect your model\n",
    "    column_types=column_types, # A dictionary with columns names of df as key and types(category, numeric, text) of columns as values\n",
    "    target='default', # The column name in df corresponding to the actual target variable (ground truth).\n",
    "    feature_names=list(feature_types.keys()), # List of the feature names of prediction_function\n",
    "    classification_labels=clf_logistic_regression.classes_ ,  # List of the classification labels of your prediction\n",
    "    model_name='logistic_regression_global_augmented_data', # Name of the model\n",
    "    dataset_name='test_data_global_augmented_data' # Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 721. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_df(\n",
    "    df=train_data, # The dataset you want to upload\n",
    "    column_types=column_types, # All the column types without the target\n",
    "    name=\"train_data_global_augmented_data\"# Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Giskard platform we get for the original model:\n",
    "Accuracy on the original test set: 0.76\n",
    "F1 score on the original test set: 0.83\n",
    "Accuracy difference : 0.01\n",
    "F1 difference : 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Giskard platform we get for the first method:\n",
    "Accuracy on the original test set:  0.8\n",
    "F1 score on the original test set: 0.86\n",
    "Accuracy difference : 0.04\n",
    "F1 difference : 0.03\n",
    "\n",
    "Higher overfitting but it remains small overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd method - Low performance features data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use oversampling heuristic methods rather than undersampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rows_random(df,rows_nb,column_name,value):\n",
    "    \"\"\"\n",
    "    The idea is to select a random subset from the 1000 original rows and replace a specific column with a specific value \n",
    "    to get more data for the low performance slices\n",
    "    \"\"\"\n",
    "    existing_df = df.copy() #Copy\n",
    "    \n",
    "    selRows = existing_df[existing_df[column_name] == value ].index #Select rows ID with the specific value\n",
    "    existing_df = existing_df.drop(selRows, axis=0) #Remove rows with the specific value\n",
    "    \n",
    "    existing_df.reset_index(drop=True, inplace=True) #Index reset\n",
    "    random_index_list = random.sample(range(1, 1000-len(selRows)), rows_nb) #Random index generator\n",
    "    new_data = existing_df.iloc[random_index_list] #Subset selection\n",
    "    new_data[column_name]= value #New value attribution\n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rows_random_2(df,rows_nb,column_name,value,column_name_2,value_2):\n",
    "    \"\"\"\n",
    "    This function is the same as pick_rows_random but with 2 features\n",
    "    \"\"\"\n",
    "    existing_df = df.copy()\n",
    "    \n",
    "    selRows = existing_df[(existing_df[column_name] == value) & (existing_df[column_name_2] == value_2) ].index\n",
    "    existing_df = existing_df.drop(selRows, axis=0)\n",
    "    \n",
    "    existing_df.reset_index(drop=True, inplace=True)\n",
    "    random_index_list = random.sample(range(1, 1000-len(selRows)), rows_nb)\n",
    "    new_data = existing_df.iloc[random_index_list]\n",
    "    new_data[column_name]= value\n",
    "    new_data[column_name_2]= value_2\n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick_rows_random(credit,250,\"credit_history\",\"all credits at this bank paid back duly\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked for each low performance slices on Giskard Inspector by analysing SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423/585379592.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name]= value\n",
      "/tmp/ipykernel_1423/585379592.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name_2]= value_2\n"
     ]
    }
   ],
   "source": [
    "no_savings_account_augmentation = pick_rows_random_2(credit,50,\"savings\",\"unknown/ no savings account\",\n",
    "                                                   \"credit_history\",\"all credits at this bank paid back duly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423/585379592.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name]= value\n",
      "/tmp/ipykernel_1423/585379592.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name_2]= value_2\n"
     ]
    }
   ],
   "source": [
    "personal_status_augmentation = pick_rows_random_2(credit,50,\"savings\",\"unknown/ no savings account\",\n",
    "                                                 \"personal_status\",\"divorced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423/585379592.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name]= value\n",
      "/tmp/ipykernel_1423/585379592.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name_2]= value_2\n"
     ]
    }
   ],
   "source": [
    "duration_in_month_augmentation = pick_rows_random_2(credit,100,\"duration_in_month\",36,\n",
    "                                                   \"personal_status\",\"divorced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423/585379592.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name]= value\n",
      "/tmp/ipykernel_1423/585379592.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name_2]= value_2\n"
     ]
    }
   ],
   "source": [
    "account_check_status_augmentation = pick_rows_random_2(credit,50,\"account_check_status\",\"< 0 DM\",\n",
    "                                                      \"default\",\"Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1423/771578641.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data[column_name]= value\n"
     ]
    }
   ],
   "source": [
    "augmented_data = pd.concat([credit,\n",
    "                            pick_rows_random(credit,100,\"purpose\",\"(vacation - does not exist?)\"),\n",
    "                            no_savings_account_augmentation,\n",
    "                            personal_status_augmentation,\n",
    "                            duration_in_month_augmentation,\n",
    "                            account_check_status_augmentation\n",
    "                           ],\n",
    "                           ignore_index=True)\n",
    "# augmented_data = credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 800. It is available at http://localhost:19000 \n",
      "Model successfully uploaded to project key 'credit_scoring' with ID = 801. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(801, 800)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "Y=augmented_data['default']\n",
    "X= augmented_data.drop(columns=\"default\")\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.20,random_state = 30, stratify = Y)\n",
    "# Fit and score your model\n",
    "clf_logistic_regression.fit(X_train, Y_train)\n",
    "clf_logistic_regression.score(X_test, Y_test)\n",
    "# Prepare data to upload on Giskard\n",
    "train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "test_data= pd.concat([X_test, Y_test ], axis=1)\n",
    "\n",
    "credit_scoring.upload_model_and_df(\n",
    "    prediction_function=clf_logistic_regression.predict_proba, # Python function which takes pandas dataframe as input and returns probabilities for classification model OR returns predictions for regression model\n",
    "    model_type='classification', # \"classification\" for classification model OR \"regression\" for regression model\n",
    "    df=test_data, # the dataset you want to use to inspect your model\n",
    "    column_types=column_types, # A dictionary with columns names of df as key and types(category, numeric, text) of columns as values\n",
    "    target='default', # The column name in df corresponding to the actual target variable (ground truth).\n",
    "    feature_names=list(feature_types.keys()), # List of the feature names of prediction_function\n",
    "    classification_labels=clf_logistic_regression.classes_ ,  # List of the classification labels of your prediction\n",
    "    model_name='logistic_regression_augmented_data_detailed', # Name of the model\n",
    "    dataset_name='test_data_augmented_data_detailed' # Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathro/.local/lib/python3.10/site-packages/giskard/client/project.py:639: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 836. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "836"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_df(\n",
    "    df=train_data, # The dataset you want to upload\n",
    "    column_types=column_types, # All the column types without the target\n",
    "    name=\"train_data_augmented_data_detailed\"# Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Giskard platform we get:\n",
    "Accuracy on the original test set:  0.84\n",
    "F1 score on the original test set: 0.89\n",
    "Accuracy difference : 0.08\n",
    "F1 difference : 0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method with global data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy : 5% improvement\n",
    "### F1-score : 4% improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Method with global data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy : 10% improvement\n",
    "### F1-score : 7% improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful with overfitting but the results are encouraging"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "German_credit_scoring_giskard (2).ipynb",
   "provenance": []
  },
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6e7ea85d-f19e-4d05-90a4-44b7668fd037",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
